{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary packages, and computer  V(vocabulary size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import collections\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import bigrams, trigrams\n",
    "\"\"\" Author: Jiyu Yan\n",
    "    Student ID: 1851015\n",
    "    For 918 exercise1\n",
    "\"\"\"\n",
    "\n",
    "def number_v(mylist):\n",
    "    return len((set(mylist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used for lemmatization.It will return the word itself If it could't find the word's tag or the word is' am, is, are'. (So it's suitable to finish the mission of generating a sentence starts with' is this'.) Receive a string and return a list after lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(str):\n",
    "    data_list = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    after_tag = nltk.pos_tag(str.split())\n",
    "    for word, tag in after_tag:\n",
    "        if tag.startswith('N'):\n",
    "            after_lemma = lemmatizer.lemmatize(word, pos=wordnet.NOUN)\n",
    "        elif tag.startswith('VBZ'):   # Don't change 'is' to 'be'\n",
    "            after_lemma = word\n",
    "        elif tag.startswith('VBP'):\n",
    "            after_lemma = word\n",
    "        elif tag.startswith('V'):\n",
    "            after_lemma = lemmatizer.lemmatize(word, pos=wordnet.VERB)\n",
    "        elif tag.startswith('J'):\n",
    "            after_lemma = lemmatizer.lemmatize(word, pos=wordnet.ADJ)\n",
    "        elif tag.startswith('R'):\n",
    "            after_lemma = lemmatizer.lemmatize(word, pos=wordnet.ADV)\n",
    "        else:\n",
    "            after_lemma = word\n",
    "        data_list.append(after_lemma)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since no more FreqDist, I implemented my own basicly same function based on dict() to compute the frequency of items. This freqdic() function could return all items with its frequency as a [key:item, value:frequency] dict(). The frequency will be 0 if it can't find the key.(That's why I use defaultdict and lambda: 0) . The second function return a sorted dict() which the most common one would be the first sorted by frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freqdic(mylist):\n",
    "    mydict = collections.defaultdict(lambda: 0)\n",
    "    for key in mylist:\n",
    "        mydict[key] += 1\n",
    "    return mydict\n",
    "\n",
    "\n",
    "def most_common(mydict):\n",
    "    return sorted(mydict.items(), key=lambda d: d[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputfile function could get all words in a txt file, used for get all words in the \"positive-words.txt\" and \"negative-words.txt\". The count_p_n function could return the number of all positive and negative word in the given list using my own function freqdic()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputfile(file):\n",
    "    with open(file, 'r') as my_file:\n",
    "        words = [every_line.rstrip() for every_line in my_file]\n",
    "        return words\n",
    "\n",
    "\n",
    "def count_p_n(mylist):\n",
    "    pos_num = 0\n",
    "    neg_num = 0\n",
    "    positive = inputfile('positive-words.txt')\n",
    "    negative = inputfile('negative-words.txt')\n",
    "    p_dic = freqdic(positive)\n",
    "    n_dic = freqdic(negative)\n",
    "    for word in mylist:\n",
    "        pos_num += p_dic[word]\n",
    "        neg_num += n_dic[word]\n",
    "    return pos_num, neg_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These function could find all trigrams and bigrams in the list and return trigrams and bigrams sorted by frequency, each with its count number. Using my own freqdic() and most_common() function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_trigram_withcount(mylist): \n",
    "    trigram_withcount = most_common(freqdic(list(trigrams(mylist))))\n",
    "    return trigram_withcount\n",
    "\n",
    "def all_bigram_withcount(mylist):\n",
    "    bigram_withcount = most_common(freqdic(list(bigrams(mylist))))\n",
    "    return bigram_withcount"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This function here is used to remove every frequency near that item and return the pure sorted trigram list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_trigram(trigram_withcount):\n",
    "    trigram_list = []\n",
    "    for i in trigram_withcount:\n",
    "        trigram_list.append(i[0])\n",
    "    return trigram_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This  function could compute the language model for trigrams, and get a sentence using it. By input a trigram list(already sorted by frequency), two words at the beginning and the sentence length, you will get a sentence. It has an inner function named gen_sentence_in, every time given 2 words, it could find the most common trigram started with these 2 words and give the third word in that trigram. By setting a flag(sen_go) outside, this inner recursive function could stop when the sentence length is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sentence(tri_list, a, b, leng):\n",
    "    sen = [a, b]\n",
    "    sen_go = True\n",
    "\n",
    "    def gen_sentence_in(tri_list, a, b, leng):\n",
    "        nonlocal sen_go\n",
    "        nonlocal sen\n",
    "        if sen_go is False:\n",
    "            return sen\n",
    "        for i in tri_list:\n",
    "            if i[0] == a and i[1] == b and sen_go:\n",
    "                sen.append(i[2])\n",
    "                if len(sen) == leng:\n",
    "                    sen_go = False\n",
    "                gen_sentence_in(tri_list, b, i[2], leng)\n",
    "\n",
    "    gen_sentence_in(tri_list, a, b, leng)\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function could compute the perplexity by evaluating on the 'test_sent'  based on 'mylist' with simple linear interpolation. When an unknown word appear(even could't find unigram), we set a P(unk) which is equals to 1/V. V is the vocabulary of 'test_sent' here.  When computing I made a reduction of fractions to a common denominator and then use a log(10) function to avoid some math bug. c3 is the count of trigram and c2 for bigram, c1 for unigram. Empirically and intuitively found linear weights a1-a4 here.\n",
    "\n",
    "**Pesti(w3|w1w2) = a1\\*P(w3|w1w2)+a2\\*P(w3|w2)+a3\\*P(w3)+a4\\*P(unk)**  \n",
    "P(w3|w1w2) = c3/c2  when c3>0.\n",
    "P(w3|w2) = c2/c1  when c2>0.\n",
    "P(w3) = c1/V   when c1>0.\n",
    "P(unk) = 1/V  when c1 = 0.\n",
    "a1+a2+a3+a4 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplex(test_sent, mylist):\n",
    "    tri_sent = list(trigrams(test_sent))\n",
    "    ftri = freqdic(list(trigrams(mylist)))\n",
    "    fbi = freqdic(list(bigrams(mylist)))\n",
    "    fui = freqdic(mylist)\n",
    "    log_tri_count = 0\n",
    "    log_bi_count = 0\n",
    "    v = number_v(mylist)\n",
    "    a1 = 0.33\n",
    "    a2 = 0.33\n",
    "    a3 = 0.33\n",
    "    a4 = 0.01\n",
    "    for tri in tri_sent:\n",
    "        c3 = ftri[tri]\n",
    "        c2 = fbi[tri[1:]]\n",
    "        c1 = fui[tri[2]]\n",
    "        if c3 != 0:\n",
    "            tri_temp = c3*a1*c1*v*v + v*v*c2*c2*a2 + a3*c1*c1*c2*v + a4*c1*c2*v\n",
    "            bi_temp = c1*c2*v*v\n",
    "        elif c3 == 0 and c2 != 0:\n",
    "            tri_temp = a2*c2*v*v + a3*c1*c1*v + a4*c1*v\n",
    "            bi_temp = c1*v*v\n",
    "        elif c3 == 0 and c2 == 0 and c1 != 0:\n",
    "            tri_temp = a3*c1 + a4\n",
    "            bi_temp = v\n",
    "        elif c1 == 0:\n",
    "            tri_temp = a4\n",
    "            bi_temp = v\n",
    "        log_tri_temp = math.log(tri_temp, 10)\n",
    "        log_tri_count += log_tri_temp\n",
    "        log_bi_temp = math.log(bi_temp, 10)\n",
    "        log_bi_count += log_bi_temp\n",
    "    p = log_tri_count - log_bi_count\n",
    "    log_pp = (-1*p) /(len(test_sent))\n",
    "    pp = pow(10, log_pp)\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function. Finished all partA and compute partB(3,4)(number of positive and negative words and stories) here. \n",
    "\n",
    "The data_list_all is a list including all data after all text preprocessing and lemmatization, used for partB. It's simply the sum of data_16000 and data_3000.\n",
    "Data_16000 means those data from first 16000 rows. Data_3000 means reamining rows. They are used for partC.\n",
    "In my own test, it costs around 300 seconds here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_16000 = []\n",
    "    data_3000 = []\n",
    "    p_num_all, n_num_all = 0, 0\n",
    "    pos_sto, neg_sto = 0, 0\n",
    "    count_line = 0\n",
    "    pattern_no_url = re.compile(r'(https://|http://|www\\.)+(\\w|\\.|/|\\?|=|&|%)*\\b')\n",
    "    pattern_no_num = re.compile(r'[a-z0-9]*[a-z][a-z0-9]*')\n",
    "    pattern_no_1letter = re.compile(r'[a-z0-9]{2,}')\n",
    "    with open('signal-news1.jsonl', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            content = json.loads(line).get('content').lower()\n",
    "            content_no_url = pattern_no_url.sub('', content)\n",
    "            content_no_num = ' '.join(pattern_no_num.findall(content_no_url))\n",
    "            content_no_1 = ' '.join(pattern_no_1letter.findall(content_no_num))\n",
    "            content_alldone_list = lemma(content_no_1)\n",
    "            if count_line < 16000:\n",
    "                data_16000.extend(content_alldone_list)\n",
    "            else:\n",
    "                data_3000.extend(content_alldone_list)\n",
    "            count_line += 1\n",
    "            p_n_num = count_p_n(content_alldone_list)\n",
    "            p_num_all += p_n_num[0]\n",
    "            n_num_all += p_n_num[1]\n",
    "            p_n_dif = p_n_num[0] - p_n_num[1]\n",
    "            if p_n_dif > 0:\n",
    "                pos_sto += 1\n",
    "            elif p_n_dif < 0:\n",
    "                neg_sto += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PartB. Compute N and V and list the top 25 trigrams here. The number of positive and negative words and the number of positive and negative stories are computed above so here just print them.\n",
    "In my own test here costs less than 10 seconds.(Already computed positive and negtive questions above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N:5774742\n",
      "V:93025\n",
      "[('one', 'of', 'the'), ('on', 'share', 'of'), ('day', 'move', 'average'), ('on', 'the', 'stock'), ('as', 'well', 'as'), ('in', 'research', 'report'), ('in', 'research', 'note'), ('the', 'year', 'old'), ('for', 'the', 'quarter'), ('average', 'price', 'of'), ('research', 'report', 'on'), ('the', 'united', 'state'), ('the', 'end', 'of'), ('research', 'note', 'on'), ('share', 'of', 'the'), ('be', 'able', 'to'), ('in', 'report', 'on'), ('earnings', 'per', 'share'), ('buy', 'rating', 'to'), ('cell', 'phone', 'plan'), ('phone', 'plan', 'detail'), ('accord', 'to', 'the'), ('of', 'the', 'company'), ('move', 'average', 'price'), ('appear', 'first', 'on')]\n",
      "number of positive words in corpus: 182365, negative words: 145652\n",
      "number of positive stories: 10588, negative stories: 6813\n"
     ]
    }
   ],
   "source": [
    "    data_list_all = data_16000 + data_3000\n",
    "    print('N:%d' % len(data_list_all))\n",
    "    print('V:%d' % number_v(data_list_all))   \n",
    "    tri_withcount_all = most_common(freqdic(list(trigrams(data_list_all))))[:25]\n",
    "    tri_list_all = all_trigram(tri_withcount_all)\n",
    "    print(tri_list_all)\n",
    "\n",
    "    print('number of positive words in corpus: %d, negative words: %d' % (p_num_all, n_num_all))\n",
    "    print('number of positive stories: %d, negative stories: %d' % (pos_sto, neg_sto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PartC. \n",
    "In my own test it costs around 20 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'this', 'the', 'company', 'has', 'market', 'capitalization', 'of', 'billion', 'and']\n",
      "perplexity of remaining rows is 28.84317604653077\n"
     ]
    }
   ],
   "source": [
    "    tri_withcount_16000 = all_trigram_withcount(data_16000)\n",
    "    tri_list_16000 = all_trigram(tri_withcount_16000)\n",
    "    sen = gen_sentence(tri_list_16000, 'is', 'this', 10)\n",
    "    print(sen)\n",
    "\n",
    "    pp = perplex(data_3000, data_16000)\n",
    "    print('perplexity of remaining rows is ' + str(pp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
