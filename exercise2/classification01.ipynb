{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanch\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import testsets\n",
    "import evaluation\n",
    "import twokenize\n",
    "import sklearn.feature_extraction\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import textPreprocessor01\n",
    "import nltk\n",
    "from nltk.stem import *\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "from gensim.models import word2vec\n",
    "import word2vecReader\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load training data\n",
    "def read_training_data(training_data):\n",
    "  id_gts = {}\n",
    "  with open(training_data, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "      fields = line.split('\\t')\n",
    "      tweetid = fields[0]\n",
    "      gt = fields[1]\n",
    "      content = fields[2].strip()\n",
    "      id_gts[tweetid] = gt, content\n",
    "\n",
    "  return id_gts\n",
    "traindic = read_training_data('twitter-training-data1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perprocessing(tdic):\n",
    "    new_dic = {}\n",
    "    for line in tdic:\n",
    "        id = line\n",
    "        gt = tdic[line][0]\n",
    "        raw = ' '.join(twokenize.tokenizeRawTweetText(tdic[line][1]))\n",
    "        text = twokenize.normalizeTextForTagger(raw)\n",
    "        text_tk = twokenize.tokenize(text)\n",
    "        # print(text_tk)\n",
    "        newtext = ' '.join(text_tk)\n",
    "        newtext = textPreprocessor01.replaceall(newtext)\n",
    "        new_dic[id] = gt, newtext\n",
    "        # print(type(tdic[line][1]))\n",
    "        # print(line)\n",
    "        # print(type(line))\n",
    "        # print(type(newtext))\n",
    "        # print(newtext)\n",
    "    return new_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_corpus(new_dic):\n",
    "    traincorpus = []\n",
    "    for line in new_dic:\n",
    "        # print('after perprocessing')\n",
    "        # print(line)\n",
    "        # print(new_dic[line])\n",
    "        # print(new_dic[line][1])\n",
    "        traincorpus.append(new_dic[line][1])\n",
    "    return traincorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_corpus(new_dic):\n",
    "    split_traincorpus = []\n",
    "    for line in new_dic:\n",
    "        split_traincorpus.append(new_dic[line][1].split())\n",
    "    return split_traincorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO extract features\n",
    "def get_ngrams(corpus):\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    # vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    # print(vectorizer.vocabulary_)\n",
    "    # print(vectorizer.vocabulary_.keys())\n",
    "    # print('ngram----')\n",
    "    # print(X.todense())\n",
    "    # print(len(X.todense()))\n",
    "    # X.todense()\n",
    "    # print(X.toarray())\n",
    "    return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(corpus):\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    # vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    # print(vectorizer.vocabulary_)\n",
    "    # print(vectorizer.vocabulary_.keys())\n",
    "    tfidf = TfidfVectorizer(vocabulary=list(vectorizer.vocabulary_.keys()), stop_words='english')\n",
    "    # tfidf = TfidfVectorizer(vocabulary=list(vectorizer.vocabulary_.keys()), stop_words='english', ngram_range=(1, 2))\n",
    "    tfs = tfidf.fit_transform(corpus)\n",
    "    # print('tfidf------------------------------------')\n",
    "    # print(tfs.todense())\n",
    "    # print(len(tfs.todense()))\n",
    "    return tfs.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding2(split_corpus):\n",
    "    print('word embedding2 --------------------')\n",
    "    all = []\n",
    "    for i in split_corpus:\n",
    "        model = word2vec.Word2Vec([i], min_count=1)\n",
    "        # print(model.vocabulary)\n",
    "        # print(model.wv.vocab)\n",
    "\n",
    "        s = model.wv.syn0\n",
    "\n",
    "        # print(s)\n",
    "        # print(len(i))   # i is each tweet.  len here is 20,25,22,14\n",
    "        # print(len(s))  # len here is 22 30 34 28. why the length here is not equal to original tweet.\n",
    "        ans = list(map(sum, zip(*s)))  # sum of them\n",
    "        # print(ans)\n",
    "        # print(len(ans))\n",
    "        all.append(ans)\n",
    "    # print(len(all))  # 4\n",
    "    # print(all)\n",
    "    return all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senti_bi_lexicon(split_corpus):\n",
    "    def inputfile(file):\n",
    "        with open(file, 'r') as my_file:\n",
    "            words = [every_line.rstrip() for every_line in my_file]\n",
    "            return words\n",
    "\n",
    "    def count_p_n(mylist):\n",
    "        pos_num = 0\n",
    "        neg_num = 0\n",
    "        positive = inputfile('positive-words.txt')\n",
    "        negative = inputfile('negative-words.txt')\n",
    "        p_dic = FreqDist(positive)\n",
    "        n_dic = FreqDist(negative)\n",
    "        for word in mylist:\n",
    "            pos_num += p_dic[word]\n",
    "            neg_num += n_dic[word]\n",
    "        return pos_num, neg_num\n",
    "\n",
    "    P_N = []\n",
    "    for line in split_corpus:\n",
    "        p_num_all = n_num_all = 0\n",
    "        p_n_num = count_p_n(line)\n",
    "        p_num_all += p_n_num[0]\n",
    "        n_num_all += p_n_num[1]\n",
    "        # p_n_dif = p_n_num[0] - p_n_num[1]\n",
    "        # if p_n_dif > 0:\n",
    "        #     P_N.append(1)  # positive\n",
    "        # elif p_n_dif < 0:\n",
    "        #     P_N.append(-1)  # negative\n",
    "        # else:\n",
    "        #     P_N.append(0)  # neutral\n",
    "        P_N.append([p_num_all, n_num_all])\n",
    "    # print('senti_binary_lex--------------------')\n",
    "    # print(P_N)\n",
    "    # print(len(P_N))\n",
    "    return P_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdic = perprocessing(traindic)\n",
    "train_corpus = get_train_corpus(newdic)\n",
    "split_corpus = get_split_corpus(newdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training myclassifier1\n",
      "twitter-test1.txt (myclassifier1): 0.538\n",
      "            positive  negative  neutral\n",
      "positive    0.698     0.077     0.225     \n",
      "negative    0.227     0.489     0.284     \n",
      "neutral     0.292     0.121     0.587     \n",
      "\n",
      "twitter-test2.txt (myclassifier1): 0.499\n",
      "            positive  negative  neutral\n",
      "positive    0.745     0.052     0.203     \n",
      "negative    0.339     0.399     0.261     \n",
      "neutral     0.436     0.082     0.483     \n",
      "\n",
      "twitter-test3.txt (myclassifier1): 0.516\n",
      "            positive  negative  neutral\n",
      "positive    0.676     0.091     0.233     \n",
      "negative    0.211     0.425     0.364     \n",
      "neutral     0.351     0.100     0.549     \n",
      "\n",
      "Training myclassifier2\n",
      "twitter-test1.txt (myclassifier2): 0.538\n",
      "            positive  negative  neutral\n",
      "positive    0.698     0.077     0.225     \n",
      "negative    0.227     0.489     0.284     \n",
      "neutral     0.292     0.121     0.587     \n",
      "\n",
      "twitter-test2.txt (myclassifier2): 0.499\n",
      "            positive  negative  neutral\n",
      "positive    0.745     0.052     0.203     \n",
      "negative    0.339     0.399     0.261     \n",
      "neutral     0.436     0.082     0.483     \n",
      "\n",
      "twitter-test3.txt (myclassifier2): 0.516\n",
      "            positive  negative  neutral\n",
      "positive    0.676     0.091     0.233     \n",
      "negative    0.211     0.425     0.364     \n",
      "neutral     0.351     0.100     0.549     \n",
      "\n",
      "Training myclassifier3\n",
      "twitter-test1.txt (myclassifier3): 0.538\n",
      "            positive  negative  neutral\n",
      "positive    0.698     0.077     0.225     \n",
      "negative    0.227     0.489     0.284     \n",
      "neutral     0.292     0.121     0.587     \n",
      "\n",
      "twitter-test2.txt (myclassifier3): 0.499\n",
      "            positive  negative  neutral\n",
      "positive    0.745     0.052     0.203     \n",
      "negative    0.339     0.399     0.261     \n",
      "neutral     0.436     0.082     0.483     \n",
      "\n",
      "twitter-test3.txt (myclassifier3): 0.516\n",
      "            positive  negative  neutral\n",
      "positive    0.676     0.091     0.233     \n",
      "negative    0.211     0.425     0.364     \n",
      "neutral     0.351     0.100     0.549     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier in ['myclassifier1', 'myclassifier2', 'myclassifier3']: # You may rename the names of the classifiers to something more descriptive\n",
    "    if classifier == 'myclassifier1':\n",
    "        print('Training ' + classifier)\n",
    "        # TODO: extract features for training classifier1\n",
    "        # TODO: train sentiment classifier1\n",
    "        f1 = get_ngrams(train_corpus)\n",
    "        F1 = np.array(f1)\n",
    "        f2 = get_tfidf(train_corpus)\n",
    "        F2 = np.array(f2)\n",
    "\n",
    "        f3 = senti_bi_lexicon(split_corpus)\n",
    "        F3 = np.array(f3)\n",
    "        # # print(f3)\n",
    "        # # print(np.array(f3))\n",
    "        # # wordembedding(split_corpus)\n",
    "        # f4 = word_embedding2(split_corpus)\n",
    "        # # F4 = np.array(f4)\n",
    "        #\n",
    "        # t1_f3 = senti_bi_lexicon(t1s_corpus)\n",
    "        # t1_F3 = np.array(t1_f3)\n",
    "        #\n",
    "        X = F3\n",
    "        #\n",
    "        labels_to_array = {\"positive\": 1, \"negative\": -1, \"neutral\": 0}\n",
    "        labels = [labels_to_array[newdic[tweet][0]] for tweet in newdic]\n",
    "        # print(labels)\n",
    "        Y = np.array(labels)\n",
    "        #\n",
    "        model = GaussianNB()\n",
    "        model.fit(X, Y)\n",
    "        #\n",
    "        # # print(clf.predict([[]]))\n",
    "        #\n",
    "        # ans_num = clf.predict(t1_F3)\n",
    "        # # print(ans)\n",
    "        # # print(len(ans))\n",
    "        # array_to_labels = {1:\"positive\" ,  -1 :\"negative\",  0:\"neutral\"}\n",
    "        # labels = [array_to_labels[i] for i in ans_num]\n",
    "        # # print(labels)\n",
    "        # # ans_dic = {}\n",
    "        # ans_dictionary = dict(zip(list(test1dic.keys()), labels))\n",
    "        # print(ans_dictionary)\n",
    "\n",
    "        # num_right = 0\n",
    "        # for count,i in enumerate(newdic):\n",
    "        #     # print(i)\n",
    "        #     # print(count)\n",
    "        #     # print(newdic[i][0])\n",
    "        #     # print(type(sentlist[count]))\n",
    "        #     if sentlist[count] == 1 and newdic[i][0] == 'positive':\n",
    "        #         # print('right ans')\n",
    "        #         num_right += 1\n",
    "        #     if sentlist[count] == 0 and newdic[i][0] == 'neutral':\n",
    "        #         num_right += 1\n",
    "        #     if sentlist[count] == -1 and newdic[i][0] == 'negative':\n",
    "        #         num_right += 1\n",
    "        # print(num_right/len(sentlist))\n",
    "\n",
    "\n",
    "\n",
    "    elif classifier == 'myclassifier2':\n",
    "        print('Training ' + classifier)\n",
    "        # TODO: extract features for training classifier2\n",
    "        # TODO: train sentiment classifier2\n",
    "    elif classifier == 'myclassifier3':\n",
    "        print('Training ' + classifier)\n",
    "        # TODO: extract features for training classifier3\n",
    "        # TODO: train sentiment classifier3\n",
    "\n",
    "    for testset in testsets.testsets:\n",
    "        # TODO: classify tweets in test set\n",
    "        test = read_training_data(testset)\n",
    "\n",
    "        testdic = perprocessing(test)\n",
    "        t_corpus = get_train_corpus(testdic)\n",
    "        ts_corpus = get_split_corpus(testdic)\n",
    "\n",
    "        t_f3 = senti_bi_lexicon(ts_corpus)\n",
    "        t_F3 = np.array(t_f3)\n",
    "\n",
    "        ans_num = model.predict(t_F3)\n",
    "        # # print(ans)\n",
    "        # # print(len(ans))\n",
    "        array_to_labels = {1:\"positive\" ,  -1 :\"negative\",  0:\"neutral\"}\n",
    "        labels = [array_to_labels[i] for i in ans_num]\n",
    "        # # print(labels)\n",
    "        # # ans_dic = {}\n",
    "        predictions = dict(zip(list(testdic.keys()), labels))\n",
    "        # print(ans_dictionary)\n",
    "\n",
    "\n",
    "        # predictions = {'163361196206957578': 'neutral', '768006053969268950': 'neutral', '742616104384772304': 'neutral', '102313285628711403': 'neutral', '653274888624828198': 'neutral'} # TODO: Remove this line, 'predictions' should be populated with the outputs of your classifier\n",
    "        # predictions = ans_dictionary\n",
    "        evaluation.evaluate(predictions, testset, classifier)\n",
    "\n",
    "        evaluation.confusion(predictions, testset, classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
